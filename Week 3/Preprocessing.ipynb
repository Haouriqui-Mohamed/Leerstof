{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d251577",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "In het vorige vak heb je reeds gezien hoe je met behulp van python en pandas de dataset kan transformeren naar een bruikbaarder formaat.\n",
    "Daarbij hadden we ook gezien dat dit kon toegevoegd worden in een pipeline zodat alles binnen het SkLearn platform gehouden kon worden.\n",
    "Het voordeel van het opstellen van een pijplijn is dat alle nodige preprocessing stappen mee opgeslagen en gedeployed worden.\n",
    "Dus moet de nodige code voor het voorbereiden van de data niet overgezet worden van de trainingscode naar de productiecode.\n",
    "Dit vermijdt bugs en zorgt voor een kleiner verschil tussen de performantie van het model in het labo/development en productie.\n",
    "\n",
    "Het is dus duidelijk dat het gebruik van een pijplijn voor je data om te zetten naar een voorspelling een belangrijk concept is binnen Machine Learning/Data Science.\n",
    "Hierdoor is het dus evident dat dit concept ook binnen tensorflow mogelijk is.\n",
    "De volgende opties worden hiervoor aangeboden:\n",
    "* Door de preprocessing stappen/lagen onderdeel te maken van het model\n",
    "    * Via het sequential model \n",
    "    * Via de function API\n",
    "* Door gebruik te maken van het Dataset object concept binnen tensorflow om de preprocessing stappen eerst te doen en de kant en klare batches aan het model te presenteren.\n",
    "\n",
    "Er zijn twee manieren om de preprocessing stappen deel te laten uitmaken van het model.\n",
    "Ten eerste is het nodig om je te herinneren dat een model in tensorflow bestaat uit een aantal lagen die achter elkaar uitgevoerd worden.\n",
    "Deze laggen kunnen voorsteld worden als een graaf. \n",
    "\n",
    "Er zijn twee manieren om om te gaan met deze lagen.\n",
    "Ten eerste kan je gebruik maken van een sequentie model.\n",
    "Hierbij moeten al je inputs op dezelfde manier behandeld worden.\n",
    "Dit kan bijvoorbeeld gebeuren als je data reeds gepreprocessed is of als het enkel numerieke waarden bevat.\n",
    "Ook als het volledige uit tekst bestaat (zoals een tweet) kan je dit model gebruiken.\n",
    "Indien je echter complexere zaken wilt doen zoals een set features normalizeren en een andere set features encoderen met one-hot encoding, dan moet je gebruik maken van de functional API om een acyclische graaf op te stellen van de uit te voeren stappen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5181ab6",
   "metadata": {},
   "source": [
    "## With Sequential model\n",
    "\n",
    "Laten we in eerste instantie eerst kijken naar hoe je de verschillende preprocessing stappen kan gebruiken in het sequentieel model.\n",
    "\n",
    "**Numerieke data**\n",
    "\n",
    "Om te werken met numerieke data zijn er de volgende mogelijkheden:\n",
    "* Normalizatie: zet het om naar een standaardverdeling met gemiddelde 0 en standaardafwijking 1\n",
    "* Discretizatie: zet continue waarden om naar discrete/gehele getallen/categorieÃ«n\n",
    "\n",
    "Hieronder staat een voorbeeld hoe deze lagen kunnen toegepast worden op een dataset met numeriek gegevens (pixelwaarden) van 50.000 figuren van 32 pixels op 32. Let op dat voor je dit door een normalizatie laag kunt sturen, dat je ervoor moet zorgen dat de tensor vlak is.\n",
    "Dit wil zeggen dat de tensor slechts uit 1 dimensie bestaat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b37df23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3780d20",
   "metadata": {},
   "source": [
    "Deze aangemaakte lagen kunnen dat toegevoegd worden aan een sequentieel model als volgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac8c668",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87eaab82",
   "metadata": {},
   "source": [
    "**Categorieke data**\n",
    "\n",
    "De preprocessing stappen die vaak uitgevoerd worden op categorieke data hebben te maken met hun encoding.\n",
    "Zo is het bijvoorbeeld nodig om:\n",
    "* Een numerieke encoding om te zetten naar one-hot, multi-hot of andere voorstellingen. Dit kan met de CategoryEncoding laag.\n",
    "* Hashing uit te voeren op een categorieke feature om een vector te bekomen\n",
    "* Een tekstuele categorieke waarde te encoderen zodat het door een neuraal netwerk gelezen kan worden\n",
    "*  Een integer waarde om te zetten naar een representatie dat gelezen kan worden door een neuraal netwerk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd33d3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9975578",
   "metadata": {},
   "source": [
    "**Welke kolom wordt gebruikt voor de onbekenden?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff361a6",
   "metadata": {},
   "source": [
    "**Tekstuele data**\n",
    "\n",
    "Hoe we tot nu toe gewerkt hebben met tekstuele data (zoals een tweet) was om er een bag of words van te maken.\n",
    "Op basis van deze bag of words kon dan ook een tf-idf waarde berekend worden om rekening te houden met heel vaak voorkomende woorden.\n",
    "Bij tensorflow kan je iets gelijkaardigs doen door gebruik te maken van de **TextVectorization** laag.\n",
    "Deze laag krijgt een volledige string mee en kan deze standaardizeren of splitsen op basis van een karakter.\n",
    "Standaardizeren van een string houdt in dat extra spaties en leestekens verwijderd worden of dat alle hoofdletters omgezet worden naar kleine letters. Als je een andere zaken wil bereiken kan je ook een functie meegeven die opgeroepen wordt voor elke string.\n",
    "\n",
    "Verder kan de TextVectorization laag ook een bag-of-words berekenen of zelfs een tf-idf waarde berekenen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf79e65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "daa6f763",
   "metadata": {},
   "source": [
    "**Preprocessing van figuren**\n",
    "\n",
    "Aangezien computervisie een heel belangrijke toepassing is, zijn er een heel breed spectrum van lagen voorzien om te werken met beelden.\n",
    "Bij Data Augmentation worden er ook nog lagen aangehaald die in principe onder preprocessing lagen vallen maar nu nog minder relevant zijn.\n",
    "Een aantal andere belangrijke lagen zijn:\n",
    "* Resizing()\n",
    "* Rescaling()\n",
    "* Flatten()\n",
    "* ...\n",
    "\n",
    "Een voorbeeld voor te werken met dit soort data wordt getoond onder het Computer Visie gedeelte ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e485d7b4",
   "metadata": {},
   "source": [
    "## With Functional API\n",
    "\n",
    "Het probleem om te werken met het sequentiele model is dat de inputs allemaal van hetzelfde type moeten zijn. \n",
    "Dit houdt in dat ze allemaal numeriek of categoriek moeten zijn.\n",
    "In veel gevallen is dit geen probleem, bijvoorbeeld bij het verwerken van tweets of figuren. \n",
    "Indien je werkt met gestructureerde data is dit echter vaak te beperkend.\n",
    "Om deze reden kan je ook gebruik maken van de functional API.\n",
    "Zet hierbij het voorbeeld van de numerieke data om zodat het werkt met de functional API ipv het sequentiele model.\n",
    "Bekijk hiervoor ook de code op het einde van de notebook van vorige les."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9797715c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05bd1633",
   "metadata": {},
   "source": [
    "# Computer visie\n",
    "\n",
    "Het tweede deel van deze notebook staat in het teken van computer visie.\n",
    "Dit deel van Machine Learning is tegenwoordig heel populair en behaalt door middel van Deep Learning neurale netwerken heel goede resultaten bij het interpreteren van beelden en video.\n",
    "De toepassingen van Computer visie zijn omvangrijk, bijvoorbeeld:\n",
    "* Medische wereld: interpreteren scans, robots om te helpen bij kinesie therapie\n",
    "* Mobiliteit: Zelfrijdende auto's\n",
    "* Productie: Magazijnen waar robots zelf items halen of kijken hoe ze iets in elkaar moeten steken\n",
    "* Gaming: Geavanceerde bots\n",
    "* Media: Maken en detecteren van deep fakes/misinformatie\n",
    "\n",
    "In deze domeinen wordt computervisie gebruikt zowel als regressie en classificatie techniek.\n",
    "Daarnaast zijn ook transformers veel voorkomend. Transformers zijn neurale netwerken die een beeld omzetten in een ander beeld. \n",
    "Dit wordt bijvoorbeeld gebruikt voor image segmentation waarbij je kan kijken waar in de figuur welke objecten aanwezig zijn om daarop analyses te doen.\n",
    "\n",
    "Hier gaan we werken met een standaard classificatieprobleem binnen het domein van computervisie, namelijk de CIFAR10-dataset.\n",
    "Deze dataset bestaat uit 60000 32x32 kleurbeelden (50000 trainingsdata, 10000 testdata). \n",
    "Er zijn 10 mogelijke klassen in deze dataset met 6000 beelden per klasse.\n",
    "De mogelijke klassen zijn:\n",
    "* airplane \n",
    "* automobile \n",
    "* bird \n",
    "* cat \n",
    "* deer \n",
    "* dog \n",
    "* frog \n",
    "* horse \n",
    "* ship \n",
    "* truck\n",
    "\n",
    "Deze oefening gaan we uitwerken in drie stappen:\n",
    "* Data genereren\n",
    "* Data augmentation\n",
    "* Opstellen neuraal netwerk\n",
    "\n",
    "## Data generation\n",
    "\n",
    "Laten we beginnen met deze dataset te downloaden van kaggle en te unzippen. Let er hierbij op dat de trainings- en testdata in verschillende mappen geplaatst worden.\n",
    "\n",
    "In theorie kan je nu alle beelden inladen en gebruiken als data.\n",
    "Echter is dit snel een grote belasting voor het geheugen van je computer.\n",
    "Hierdoor kan het beter zijn om de data slechts in stukjes in te lezen.\n",
    "Dit gebeurd in de code hieronder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "007f5746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
      "Your Kaggle username: jensbaetensodisee\n",
      "Your Kaggle Key: Â·Â·Â·Â·Â·Â·Â·Â·\n",
      "Downloading cifar-10.zip to .\\cifar-10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 715M/715M [00:34<00:00, 21.7MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting archive .\\cifar-10/cifar-10.zip to .\\cifar-10\n"
     ]
    }
   ],
   "source": [
    "# download\n",
    "import opendatasets as od\n",
    "od.download(\"https://www.kaggle.com/competitions/cifar-10/overview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "076d712c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyunpack in c:\\users\\jens.baetens3\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.3)\n",
      "Requirement already satisfied: easyprocess in c:\\users\\jens.baetens3\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyunpack) (1.1)\n",
      "Requirement already satisfied: entrypoint2 in c:\\users\\jens.baetens3\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyunpack) (1.1)\n",
      "\n",
      "[notice] A new release of pip available: 22.1.2 -> 22.2.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "Requirement already satisfied: patool in c:\\users\\jens.baetens3\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.12)\n",
      "\n",
      "[notice] A new release of pip available: 22.1.2 -> 22.2.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "Collecting scikit-image\n",
      "  Downloading scikit_image-0.19.3-cp310-cp310-win_amd64.whl (12.0 MB)\n",
      "     --------------------------------------- 12.0/12.0 MB 27.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\jens.baetens3\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-image) (1.8.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jens.baetens3\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-image) (21.3)\n",
      "Collecting imageio>=2.4.1\n",
      "  Downloading imageio-2.21.1-py3-none-any.whl (3.4 MB)\n",
      "     ---------------------------------------- 3.4/3.4 MB 35.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\jens.baetens3\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-image) (1.23.0)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in c:\\users\\jens.baetens3\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-image) (9.2.0)\n",
      "Collecting PyWavelets>=1.1.1\n",
      "  Downloading PyWavelets-1.3.0-cp310-cp310-win_amd64.whl (4.2 MB)\n",
      "     ---------------------------------------- 4.2/4.2 MB 33.3 MB/s eta 0:00:00\n",
      "Collecting networkx>=2.2\n",
      "  Downloading networkx-2.8.6-py3-none-any.whl (2.0 MB)\n",
      "     ---------------------------------------- 2.0/2.0 MB 32.5 MB/s eta 0:00:00\n",
      "Collecting tifffile>=2019.7.26\n",
      "  Downloading tifffile-2022.8.12-py3-none-any.whl (208 kB)\n",
      "     ------------------------------------- 208.5/208.5 kB 13.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\jens.baetens3\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging>=20.0->scikit-image) (3.0.9)\n",
      "Installing collected packages: tifffile, PyWavelets, networkx, imageio, scikit-image\n",
      "Successfully installed PyWavelets-1.3.0 imageio-2.21.1 networkx-2.8.6 scikit-image-0.19.3 tifffile-2022.8.12\n",
      "\n",
      "[notice] A new release of pip available: 22.1.2 -> 22.2.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# install for handling 7z\n",
    "!pip install pyunpack\n",
    "!pip install patool\n",
    "!pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76abfa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1942e4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c36b8ca",
   "metadata": {},
   "source": [
    "## Data augmentation\n",
    "\n",
    "Het verzamelen van voldoende data voor het trainen van Deep Learning computervisie toepassingen kan heel kostelijk worden omdat er vaak heel veel figuren (tienduizenden tot miljoenen) moeten verzameld en gelabeld worden.\n",
    "Het is dus duidelijk dat dit niet ideaal is.\n",
    "\n",
    "Een mogelijkheid om de benodigde hoeveelheid voorbeelden/data/figuren te beperken is door kleine wijzigingen aan te brengen aan je bestaande data.\n",
    "Hierdoor zorg je voor meer variatie wat er voor zorgt dat je model beter kan generaliseren en de klans op overfitting verkleint.\n",
    "Hierdoor krijg je typisch betere resultaten.\n",
    "Een voorbeeld hoe je dit kan doen, zie je in onderstaande code.\n",
    "Let op dat het de bedoeling is dat deze code uiteindelijk deel uitmaakt van de totale pipeline maar hier houden we het nog apart om het resultaat van de augmentatie stappen te kunnen bepalen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890689e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c323eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0eb891c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6113d7d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1a1858c",
   "metadata": {},
   "source": [
    "## Convolutionele neurale netwerken\n",
    "\n",
    "### Convolutionele lagen\n",
    "\n",
    "### Pooling lagen\n",
    "\n",
    "### Full example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c7b6b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e58cec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "d5e8e3a19af5ceb2434683dff87da6345c3b29f7eb0a8a138558c07d014a01cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
